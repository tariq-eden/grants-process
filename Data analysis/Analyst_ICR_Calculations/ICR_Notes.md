# Context for Caluclating Inter-Coder Reliability in Qualitative Analysis

## Background
Approaches to the qualitative analysis of textual data vary along several dimensions. One common feature of these approaches is that analysis is governed by consistently applied rules for classifying passages of text by indexing (‘coding’) them in a way that allows for a robust analysis of that text (Popping 2000 p.8). For example, in summative-content analysis, classifying repeating elements of a given text using ‘codes’ can help to identify and quantify latent content and contribute to a better understanding of this text within a given context (Hsieh & Shannon, 2005, pp. 1283–1286). When applied consistently and transparently, these ‘coding’ processes can provide “a research technique for making replicable and valid inferences from [a set of] text to their content” (Popping 2000, p.7). 
However, as with any other research technique, the potential for replicability is dependent on the transparency and repeatability of the analytic process. In addition, given the complexities of textual data and the variability of the instruments used to analyse etc., replicating a given analytic process is especially difficult. As such, there is a recognised value in the re-analysis of textual data within qualitative research practices (Wästerfors, Åkerström, and Jacobsson 2014). 
In addition to re-analysis of a dataset that explores alternative approaches, there is also value in ensuring that a given coding structure can be re-used to provide a complementary analysis by different researchers. For example, in the case of textual-analysis, while classification guidelines for analysing a given set of texts are often pre-articulated in a ‘codebook’ there is often a tension between the value of top-down and bottom-up approaches to examining textual data. Regardless of the initial approach, developing a codebook that can be reliably re-used by other coders to provide similar analyses of the same data-set can increase confidence in the reliability of both the classification framework (the codebook) and the measurement instruments (the researchers doing the coding). This complements the more general notion that having multiple researchers analyse the given text provides a way to assess the intersubjectivity of the themes each analyst identifies in the data-set. In this context, evaluating the inter-coder reliability (ICR) of a coding structure is increasingly recommend in guides to qualitative analysis [for summary see ‘Inter-coder reliability in qualitative research: Debates and practical guidelines’ author tbc]. 
### What is Inter-Coder Reliability? 
The question of ICR is tied up with related yet distinct factors important for reliable and transparent analytic processes: ensuring coder-stability, developing inter-coder agreement, and then testing for reliable inter-coder consistency and coder accuracy. 
### Code stability  
Coder stability measures whether there is any change over time in how an individual researcher uses the given codes for the relevant types of data-units (Campbell et al. 2013, 295). Another way to put this is to ask how well the classification a single analyst assigns at one time point agree with their classifications (codes) for the same (or similar) text at another time point (Flight and Julious 2015).
### Coder accuracy 
Coder accuracy measures the rate at which an individual researcher can be expected to apply a given code in ways that are consistent with a pre-defined coding scheme that has previously been established to achieve high inter-coder reliability (Campbell et al. 2013, 295). 
### Inter-coder consistency 
Inter-coder consistency measures how well one researcher’s classification of a given text agrees with other analysts’ classifications of the same (or similar) texts. There are multiple approaches to this: 
### Inter-coder agreement (ICA): the extent to which different researchers tend to assign the same code to the same unit of data (Popping 2010). In one account: “Intercoder agreement requires that two or more coders are able to reconcile through discussion whatever coding discrepancies they may have for the same unit of text—discrepancies that may arise, for instance, if some coders are more knowledgeable than others about the interview subject matter” (Campbell et al. 2013, 297). Note that, although a narrower assessment than ICR/IRR, ICA is a crucial step in developing a coding structure with a high rate of ICR/IRR. For example, if there is a high ICA then calculating ICR/IRR can offer an indication of the extent to which the variance in the coding by different researchers is attributable to differences among the objects coded.
Inter-coder reliability (ICR) assesses the extent to which multiple researchers tend to make similar coding decisions in assessing the characteristics of text, and is measured based on the degree of agreement between how different researcher assign codes to the same text segments (MacPhail et al. 2016, 199). Agreement, in this context, “requires that two or more equally capable coders operating in isolation from each other select the same code for the same unit of text” (Campbell et al. 2013, 297). This measure is intended to provide an indication of the extent to which the variance in the coding by different researchers is attributable to differences among the objects coded rather than idiosyncratic coding decisions (Popping 2010). This process typically includes evaluating how consistently multiple individual researches each apply ‘codes’ to the same set of texts (usually a subset of the complete sample, or a set of pilot data). Individual analyses of the test-data are compared, and the codes used by each of the researchers assessed for agreement (refining, combing, and disambiguating as needed) (Popping 2010). During this process, researchers should come to a shared understanding of which ‘units of meaning’ should be assigned which code(s) and, in complex cases (such as with ambiguous phrases or implied concepts), the decision rules must be determined and clearly documented. (Popping 2010). Often, this shared understanding will take considerable effort to achieve (depending on the complexity of the coding structure, the ambiguity in the data, and differing tendencies towards ‘splitting’ or ‘chunking’ when coding units of meaning). In some cases, adequate ICR may be determined once multiple coders are consistently allocating the same quotations to the same general group of codes (theme), even if not agreeing on each identical code for every unitized segment of data (e.g., MacPhail et al. 2016, 206). Along the way, the process of calculating ICR can also be used to identify weakness in the analytic approach, including imprecise categories for coding (MacPhail et al. 2016). In addition, the final ICR calculation can provide an indication of how difficult it will be for independent investigators to replicate the outcomes of a given analysis following the documented methodical steps (Popping 2010). While often used synonymously with IRR (and both dependent on a basic level of inter-coder agreement), the emphasis on inter-coder reliability is often used to highlight the iterative process of developing a codebook that provides the guidelines by which several trained analysts can achieve high ICR when interpreting context-dependent textual data during various stages of the qualitative analysis project. Where relevant, the final ICR is sometimes reported as an indication of how likely it is that an independent investigator could follow the codebook as an indexing guideline when analysing the data such that their interpretation of that data supported the same conclusions (i.e., as an indicator of the likely inter-subjective reliability of a given interpretation of a specific set of qualitative data). In contrast, inter-rater reliability emphasises the outcome of an analysis where two or more raters were able to reliably recognize instances of pre-defined types of textual content (typically based on key words rather than on an interpretation of the latent meaning evident in the way the content and context of the text interact). – something that is typically reported as an indication that independent coders can accurately use a predefined rubric/scale/codebook used to categorise the textual data is reliable despite any variance in the interpretations of the text by individual analysts. While appropriate in many contexts, I don’t consider IRR to be something that is relevant for assessing the inter-subjectivity of an analytic processes that involves interpretative engagement with qualitative data.  
##3 Interrater reliability (IRR): often used as a synonym for ICR, this is sometimes also used more broadly for the degree of agreement, consistency, or shared variance among two or more raters’ assessments of a given data set. IRR is limited to outcomes that are nominal or ordinal and is typically expresses as a number between 0 (no agreement) and 1 (perfect agreement) (Flight and Julious 2015; ten Hove, Jorgensen, and van der Ark 2018). This is appropriate for many survey methodologies, and horizon scanning, etc.,
While often used synonymously with ICR (and both dependent on a basic level of inter-coder agreement), the emphasis on inter-rater reliability emphasises the outcome of an analysis where two or more raters were able to reliably recognize instances of pre-defined types of textual content (typically based on key words rather than on an interpretation of the latent meaning evident in the way the content and context of the text interact). In contrast, inter-coder reliability provides a term that highlight the importance of the iterative process of developing a codebook whereby multiple analyst can achieve high ICR when interpreting context-dependent textual data during various stages of the qualitative analysis project. While appropriate in many contexts, I don’t consider IRR to be something that is relevant for assessing analytic processes that involves interpretative engagement with qualitative data.

## Calculating ICR score
There are multiple approaches to calculating an inter-coder reliability (ICR) score. These include calculations based on mean pairwise agreement, simultaneous agreement, and majority agreement (Popping 2010, 1071). There are benefits and limitations of each approach. 
For content-analysis, determining reliability requires an assessment of ICA (i.e., the extent to which coders make the identical coding decisions) rather than covariation - as such, it is important not to use correlation-based indices that standardize coder values and only measure covariation (such as Cronbach’s alpha, Pearson’s r) (Lombard, Snyder-Duch, and Bracken 2002). Using percentage agreement (# of code agreements/# of coding decisions) is also strongly discouraged more generally because it can over-estimate agreement by not allowing for agreement as a result of chance (Belur et al. 2018; Hruschka et al. 2004; MacPhail et al. 2016). 
Krippendorff’s alpha (α)
Krippendorff’s alpha corrects for chance and measures observed and expected disagreement (rather than agreement) but is computationally difficult (MacPhail et al. 2016). 
Fleiss’ K
This is a many-raters extension of Scott’s pi (Scott’s π).  
Scott’s π
“Scott’s pi is appropriate only for nominal level variables and two coders (although Craig, 1981, has suggested an extension for three or more coders).” (Lombard, Snyder-Duch, and Bracken 2002, 591)
Note: Scott’s pi was designed to work with two raters only (Steve Kambouris email correspondence Oct 2019)
Cohen’s kappa (κ)
Cohen’s kappa (κ) specifies how much agreement we would expect by chance, how much agreement over and above chance was achieved and computes the ratio of these values to find how much agreement was observed over and above what would be expected by chance. As such, κ provides a way to measure not only accuracy (getting the coding aligned with the codebook) but precision (ensuring that agreement between coders is not due to chance alone)” (Belur et al. 2018). To calculate κ, where Ao is the observed difference and Ae is the amount of agreement expected by chance, use the formula: κ= Ao-Ae1-Ae
The value ranges from -1 to 1, with values below 0 as less than chance, 0 as incidental agreement, and values above 0 indicating greater than chance agreement. Note that, while scores closer to 1 indicate better agreement, “a conclusive criterion for a kappa value that denotes sufficient agreement remains elusive” (MacPhail et al. 2016, 200).
In addition: “Several concerns have been raised about the kappa statistic, including its dependence on code frequencies and its conservative estimation of intercoder agreement (Brennan and Prediger 1981; Byrt, Biship, and Carlin 1993; Googenmoos-Holzmann 1993).” (Hruschka et al. 2004, 326)
For ordinal or interval codes, there are a number of extensions and alternatives to kappa that should also be considered (Banerjee et al. 1999)
Note: This is the method that Cordelia has used in some of studies but doesn’t necessarily recommend.
Note: Cohen’s kappa was designed to work with two raters only (Steve Kambouris email correspondence Oct 2019), 
ICC
Alex Marcoci has previously measured IRR with ICC (see a description of the methodology used their 2019 publication: https://www.frontiersin.org/articles/10.3389/fpsyg.2018.02634/full )
Alex’s advice “ On guidelines on how to use/report ICC, I found the following articles interesting: 
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4913118/
https://psycnet.apa.org/record/1979-25169-001 
In our practice in using ICC for measuring the IRR of raters evaluating quality of reasoning in narrative reports, we found that an initial calibration session is helpful. However, even with calibration, individual raters had low ICC scores. BUT: teams of at least 3 raters showed good to excellent ICC scores.” 

Note “ICCs (intra-class correlations) are only applicable to ordinal, interval, or ratio scales, since there’s an underlying assumption that the true value lies on a continuum – this isn’t really applicable to nominal categories, which are discrete. Alex et al’s study involved ratings using a rubric, in which all subscales were ordinal, so it makes sense there.” (Steve Kambouris email correspondence Oct 2019)
When to calculate ICR
There are at least three moments when ICR should be calculated (Chi-Jung Lu and Shulman 2008, 113):
During coder training: here an example sample of data-units can be used to refine the coding structure and/or coding instructions based on the initial comparison – repeating until an adequate level of agreement is reached between multiple coders.  
During a pilot test: here a representative sample of data-units are used to reassess ICR – if ICR is inadequate then additional training and/or code-structure refinement should be undertaken and an additional pilot test run. 
During coding of the full sample: only after an adequate ICR has been achieved in pilot samples should the full sample be coded. After coding, a representative sample of the full data-set should be measured for ICR – there are the measurements to be reported formally. 
Guidelines for Conducting Qualitative Analyses with high ICR/IRR
Adapted from the guidelines from Hruschka, et al., (2004) (who incorporates suggestions from MacQueen et al., (1998) among others). 
Recognise that good ICR is a necessary but not sufficient condition for generating robust analyses that can support generalisable research claims. It is equally important to attend to the other stages of the research chain required to collect quality data and conduct meaningful analyses. 
Develop and maintain a codebook that is clear and easy to follow:  
Schedule regular coding team meetings;
Develop a plan for segmenting text while codebook is being developed;
Assign one lead coder who (preferably someone with prior experience coding) who oversees maintaining the coding guidelines (including noting codebook revisions that emerge during team meetings, implementing revisions, and organising the distribution of sample-data for reliability assessments). 
In defining codes, do not assume anything is obvious. Provide explicit descriptions of each code, differential distinctions from other codes, and examples for inclusion and exclusion.  
Be prepared to throw out codes that do not work and rework problematic codes (by redefining codes to change how they split or clump analytic units of interest).
Accept that text will need to be recoded
Seek to enhance intercoder reliability by:  
establishing intercoder reliability measures early in coding process; 
coding independent subsamples at each coding round. This can reduce the possibility that increased the intercoder reliability for a code is due to previous discussions about previously-coded text segments. 
Using an intercoder reliability measure that is appropriate. Never use a simple percentage of agreement (# of code agreements/# of coding decisions) – there multiple options that take chance-agreement into account and are each more/less appropriate in different contexts.    
Pre-determining an appropriate level of reliability for the chosen measure of inter-coder reliability. For example, when conducting qualitative analyses within medical contexts, there is “rough agreement on acceptable criteria for the highest levels of intercoder agreement… using kappas ≥ 0.90-0.90 as a target…. There are benefits and costs associated with the choice of a cutoff. If one is only concerned with a rough estimate of a code’s population prevalence, a cutoff of kappa ≥ 0.8 or even kappa ≥ 0.7 could be acceptable. A disadvantage of higher cutoffs is the increased energy, time, and coding iterations that may be required to reach them. When the correct classification of individual cases is important, as in the case of clinical diagnosis or assignment to a particular treatment or intervention, stringent cutoffs (kappa ≥ 0.9) may be necessary.” (Hruschka et al. 2004, 326).
Clearly reporting the steps taken to assess and establish intercoder reliability (see Lombard et al., (2002) and Carey et al., (1996)). 
Including the code-specific kappa when presenting data on individual codes (in addition to the summary measure used to assess the overall codebook).
Cautiously interpret codes for which coders could not achieve high agreement.

## Guidelines for Reporting ICR
see Lombard et al., (2002) and Carey et al., (1996).

## Methodological considerations that impact ICR
The number of coders/raters is important, because some indices are designed to work with only two coders, while others can work with any number.
Cohen’s kappa and Scott’s pi are designed to work with two raters only.
Krippendorff’s alpha and Fleiss’ K (a many-raters extension of Scott’s pi) can work with any number of raters
For ICR, “current recommendations suggest using no more than 40 codes (Hruschka et al., 2004; MacQueen et al., 1998)” (MacPhail et al. 2016, 201). 
When it comes to ICR there are concerns about different unitization of texts (i.e., how different coders identify appropriate blocks of text for coding).
One way to balance the need to standardise unitisation of texts and the richness of coding of ‘units of meaning’ rather than specific terms/phrases is to stipulate a clearly demarcated text boundary (e.g. a paragraph) and allow coders to assign multiple codes. This approach to mandating text unit size can “ensure coders can maintain text units in context, but simultaneously increasing the chances of ensuring reliability” (MacPhail et al. 2016, 203). A complementary strategy is to err towards using more codes, rather than coding large amounts of text into a single code, as “it is easier to merge more detailed codes later during analysis than it is to separate out larger codes” (MacPhail et al. 2016, 207). 
Another difficulty with ICR is different approaches to coding units of meaning. To be calculable, the various indices of IRR need the data to be structured in a particular way. For example, MacPhail et al., (2016, 205) “found that the coders had very different ideas about when to use codes: one coder tended to use very specific codes that captured every nuance of the data, while the other often used a single code that captured a main idea of the quotation”. 
There have to be common units of meaning across all the raters (e.g. units of meaning are whole comments, or individual paragraphs within comments, or individual sentences within paragraphs). For this project, the common unit of meaning across all the analysts are the following: 
For the in-platform response, the entire text-box (approximately one to three sentences)
For the recorded group discussions, [tbc]
For the follow-up interviews, [tbc]
The “rating” or “coding” scale needs to be nominal, ordinal, interval, or ratio. For this project, the coding is for nominal categories. 
The purpose of coding should be clear. For the initial round of coding there is one purpose: exploring what heuristics people use when reasoning about the replicability of a claimed effect. For this purpose, we are prioritizing elements of reasonings (and are not intending to index affect) and are tying not to pre-suppose what elements of reasoning will be used. As such, we have included a wide range of different reasons in the codebook (chosen based on trial data) and the relevance of these is currently being calibrated (i.e. checked to see if different analysts use these codes consistently) on a subset of the actual data. As such, there is only one codebook at this stage. Having said that, we have also been asked to provide analyses of the textual data that contribute to answering a range of different research questions.  Therefore, while the initially is on one purpose, there is  an intention to allow room for both secondary analysis of the initial coding for additional research questions and re-coding of the data as appropriate.  For example, the heuristics round of coding may also be able to provide a resource for secondary analysis of the textual data to explore how different combinations of reasoning elements relate to different types of predications. Depending on the research question, the ICR for different codes will be relevant for some and not others. For example, one of the core-level of categories is ‘plausibility’, and another is ‘questionable research practices’ – each of these sits under different sorting categories (reasoning about the claim/effect, and reasoning about the documented methods/analysis respectively) and each will have child-categories (types of plausibility and types of QRPs respectively). If the research question is about how different types of plausibility are considered in relation to the replicability of an effect, the types of plausibility will be relevant but the types of QRPs will not. So, ideally, we should report the ICR on a code-level rather than on a codebook level.



Decisions on how to calculate ICR. 
- See Phase 1 Archives

Based on the literature and the specific requirements of this project, we calculated the inter-coder reliability (ICR) at multiple stages throughout the throughout the creation and calibration of the codebook rather than waiting to report a measure of inter-rater reliability post-hoc. 
This process was included to ensure that any conclusions drawn from a textual-analysis interpretation of these types of textual-data can do so using a codebook that can be implemented consistently by multiple analysts who are each required to interpret the text (not just rate responses based on a rubric). 
As such, when reporting qualitative analyses, the ICR for each node will be reported along with any conclusions that relies upon specific nodes (or combinations thereof). In addition, the minimum ICR per code required to be included in the codebook for the purposes of mixed-methods analysis was set at a Krippendorff’s alpha of 0.8.  (Note that for the reasonWAgg categories, we preregistered a more generous 0.667 Krippendorff’s alpha as a blanket cut-off – see the discussion of this included in the notes)
