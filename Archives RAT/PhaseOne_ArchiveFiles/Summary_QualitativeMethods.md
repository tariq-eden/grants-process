# repliCATS Phase One Qualitative Research Methods Summary

## Context for qualitative research decisions in the repliCATS project

To complement the quantitative research questions within the repliCATS project, we incorporated a qualitative approach to investigate the reasoning provided for predictions about the replicability of research claims.

In this context, our data included the textual data collected through the free-text responses that participants provided, along with numerical estimates, about the likely replicability of specific research claims (on the repliCATS platform variation on the IDEA protocol). This large data set was supplemented by interviews with a sub-set of the participants in repliCATS workshops. More detail on the data-collection process has been documented by out Elicitation and Aggregation teams. The following details are provided as an overview of our qualitative approach to analysing this textual data during Phase 1 of the project.

While qualitative and quantifiable approaches overlap up practice, this remains an influential way of differentiating between research involving non-numerical data, such as text and images (qualitative) and those (quantitative) approaches that focus on data that can be handled numerically (Schwandt 2001; Vogt and Johnson 2016). While studies are sometimes considered qualitative simply due to involving non-numerical data, there is a wide range of approaches to analysing textual data within the qualitative methods literature. For example, within the social sciences it is common to focus on the frequency of various analytic categories (including term use or instances of types-of-responses). Meanwhile, within the humanities, qualitative approaches extend beyond the frequency of terms to examine the contextualised content of textual data (including adapting hermeneutic approaches to interpreting texts or following phenomenological traditions that emphasise descriptive accounts of subjective experiences). [1]

Drawing on existing approaches, we incorporated contextualised content-analysis techniques for systematically interpreting textual data by developing rules for multiple analysts to consistently index (‘code’) content within passages of textual data in ways that supports convergent analyses of those texts. This approach identifies the appearance of content-types within text using a set of analytic categories (‘codes’), yet that goes beyond calculating the frequency of the application of such codes to interpret the underlying content of that text within the relevant context (Hsieh and Shannon 2005). In interpreting the implicit meaning of texts, we followed existing guidelines by combining provisional coding for predefined grammatical and elemental codes with an exploratory stage of indexing additional content of relevance to the research questions (Saldaña 2016). This process went beyond providing instructions for identifying the appearance of terms (e.g., plausible) in the text, to specify synonymous terms and guide interpretation of implicit references to the category of analysis (e.g., content relevant to the category of plausibility might include texts such as ‘reason to believe’, ‘seemed reasonable’, and so on). This type of structured approach aligns with a process for qualitative content-analysis that has been described as “a research technique for making replicable and valid inferences from [a set of] text to their content” (Popping 2000, 7).

This analytic process was formalised by developing a codebook specifying the inclusion and exclusion criteria for those codes most relevant to our research questions. To ensure the stability of these codes in terms of both consistent application by multiple analysts (inter-coder agreement, ICA) and for reliably reflecting the content of the textual data collected (inter-coder reliability, ICR) at an appropriate level of complexity, this codebook emerged through an iterative process where several different analysts independently analysed subsections of the data and then compared which codes each applied to the same sections of text. For example, when multiple analysts used a key version of the codebook to index the same subset of the texts, the ICR of each code was calculated. The ICR results contributed to reviewing the convergence of textual interpretation between analysts. In addition, when this process included analysts naive to the codebook development, ICR calculations provided an indication of which analytic categories required further calibration and which could function as a reliable tool interpreting the text in relation to our research questions for multiple analysts.

The value of transparent process and the potential of re-analysing textual data is well-recognised within qualitative research practices even though it is not relevant for all studies (Wästerfors, Åkerström, and Jacobsson 2014; Dag Stenvoll and Peter Svensson 2011). Therefore, in addition to calibrating our codebook to support multiple analysts converging upon consistent interpretations of the text, we also documented the decisions made during the codebook development and further analysis. This includes the current documentation on how the textual data was indexed, as well as the archived versions of our codebooks, to support re-analysis by those interested in our research questions. However, given the amount and richness of the textual data collected, there would also be value in re-indexing the data with the goal of examining different research questions entirely. Either way, by documenting the indexing process, we hope that any additional analyses of this textual data will have the benefit of potential cross-references with the analysis already conducted.

## Data Collection Process

The aim of collecting qualitative data during the repliCATS project was to provide a way to analyse the reasons that participants gave about the likely replicability of published research claims in the social sciences. For each participant, we collected three types of data: a) basic demographic information (quantitative and qualitative data) b) their predictions (quantitative data) and c) information about how predictive judgements were made (qualitative data). Predictions were elicited through a [technical implementation of the IDEA Protocol](https://scholarspace.manoa.hawaii.edu/handle/10125/70666). In both local and remote IDEA groups, types b) and c) were collected using a custom-built platform for group interaction. In addition, local groups had the opportunity discussing the claims in person (in addition to entering their predictions and associated reasoning into the platform). As such, workshop facilitators took detailed notes and audio recordings of group discussions (with participant’s consent).
A subset of the participants were interviewed about their own individual reflections of experiences during the assessment of specific claims following the SIPS2019 workshop and the SIPS2020 workshop (X participants) (while, due to delays with transcription, these interviews were not analysed during Phase 1, they will remain with the data set).

## Data-processing

Following a trial and several discussions exploring options, a process was agreed upon where the Data-Management team would download data from the platform and prepare excel files that could be uploaded into our Computer-Aided Qualitative Data Analysis Software (CAQDAS), NVivo version 12. While this formatting process evolved over the course of the project, each batch included four file types (also see the data [pipeline overview](https://github.com/metamelb-repliCATS/aggreCAT)):

1. J&C Files (Textual Responses by Claim): each row functions as a unit of textual data (either a justification or a comment), with a column for each of the characteristics of those responses (including a column for linking that data to the other file types).

2. Ratings Files (Numerical Responses by Claim): with one row for each participant per round per claim, columns were included for each question as well as for additional characteristics of those responses (including a column for linking that data to the other file types).

3. Claim Metadata Files: with one row per claim, columns were included for each of the characteristics of that claim (based on COS metadata for each claim: Claim id; journal; COS journal discipline; Web of science discipline).

4. Participant Demographic Files: with one row per participant, and columns for each of the demographic questions asked and each quiz-question answered.

For file types 1-3, we initially chose to batch response by claim (one file per claim) for the SIPS2019 workshop, the time required to upload each file (~1min per file) prompted us to change to batching in larger sets (e.g., by workshop).The demographics file was updated progressively with each workshop, with the full set uploaded with each batch of the other files.

Depending on the data-processing pipeline, these file types required additional preparation prior to uploading into NVivo. This included, removing any #NA and ‘test’ listings, and removing duplicate listing (for example, when participants had multiple display names over the course of the project).

For the SCORE data, there were 12 batches:

| Claim Batch | Number of Claims| Number of Textual Responses |
| :----------:| :--------------:|:--------------:|
| 1 | 557 | 10198 |
| 2 | 219 | 3459 |
| 3 | 188 | 3489 |
| 4 | 598 | 2508 |
| 5 | 30 | 447 |
| 6 | 28 | 402 |
| 7 | 300 | 3811 |
| 8 | 143 | 2019 |
| 9 | 234 | 2947 |
| 10 | 235 | 2909 |
| 11 | 235 | 2896 |
| 12 | 234 | 3087 |
| Totals | 3001 | 38172 |

In addition to the SCORE data, we also included a batch of data collected at the SIPS2019 workshops from the groups assessing ‘known-outcome claims’ rather than SCORE claims. This was collected and processed in the same way that the SCORE claims were, with some adjustments. In terms of collection, the key difference was that each claim was assessed by multiple groups. For the file types, an additional column was added in the Metadata file for the actual results of a replication of each claim. A fifth file type was also generated for the self-reported expertise of each individual about each of the claims (one row, per participant, with columns for each claim).

Once files had been uploaded into NVivo, each file (or relevant rows within files) were indexed using the Case and Case Classification functions of this CAQDAS. These function to classify multiple units of text at a given case for further analysis (e.g., to review all responses by a group across multiple claims, or all responses for a claim across multiple groups).

Within NVivo, file types 2-4 were used to create Case Classifications. For instance, for the SCORE claims, the classifications (and the cases they were linked to) were:

- Claim_Metadata (linked to the ‘claim id’ column from the J&C file)
- Ratings_NumericalResponses_by_Claim (linked to the ‘unique id participant’ column of the J&C file).
- Participant_attributes_ ( linking the demographics file to the participant id in the ‘justifier’ column of the J&C file).

These classifications were applied manually in NVivo, and given the workflow in this project, this was a separate process to the use of 'nodes' to apply analytic categories to text units. Once both case classifications and nodes were applied to text-units, queries were run within NVivo to extract relevant subsets of data relevant to specific research questions (see Qualitative Analysis Process for more detail).

For each batch of data, an NVivo Project was created with the relevant files processed and uploaded. Following recommendations in Woolf and Silver (2017, 108), for each NVivo Project a process of distribution-merge-distribution was used. The recommended steps involved in this process can be described as follows (Woolf and Silver 2017, 110):

- A 'merge-manager' creates a base NVivo project by uploading the relevant source files, 'codebook' of nodes, and naming any cases and classifications needed for linking files.
- Each analyst is trained in the use of the relevant codebook nodes, cases, and classification.
- Each analyst is sent a copy of the base NVivo file to complete a specified task (e.g., link case-classification to a file, or analyse a set of text-units using the nodes)
- Each analyst completes their task in their version of the NVivo project (the 'sub-project'), remembering to avoid adding or deleting sources or cases, or deleting or changing the names of nodes, cases, or classifications, saves their work, creates a copy of their NVivo sub-project, and sends it to the merge-manager for review.
- The merge-manager opens each submitted sub-project to confirm that each one contains all the sources in the master it was initially copied from and imports each sub-project into a master-project to be reviewed.
- An analyst meeting is held to discuss any suggested additions, modifications, or deletions to the NVivo project, as well as any sources that need to be added to the project.
- If required, the merge-manager adds any new sources to the updated master-project, makes any agreed-upon modifications, and removes any agreed-upon deletions, before copying to re-send to each analyst for the next round of analysis (and archives a copy for the project records). Once received, each analyst continues work on the newest copy of the project (archiving the old version).
- Repeat as required.

## Qualitative Analysis Process

Our principal unit of analysis was the justifications that participants provided for their prediction of the replicability of a claim. In analysing these, we explored trends in reasoning within these justifications for each claim across multiple individuals and multiple groups. In addition, we  explored trends within group-level and individual-level justifications across multiple claims.

The process of analysing this textual data involved iterative rounds of interdependent practices: data familiarising to develop analytic categories and stabalising these through training and codebook refinement; context-based interpertions of textual responses in terms of selected analytic categories; automated indexing of textual data; and querying the indexed datasets in relation to specific research questions. However, for brevity, these are summarised in a broadly chronological sense below.

### Developing analytic categories

We developed a codebook that specifies the key analytic categories used to interpret the textual data collected during the repliCATS project. The development of these analytic categories, and the iterative use of relevant versions of the codebook collecting these categories, can be summarised in terms of qualitative analytic task types (adapted from Woolf and Silver (2017).

| Analytic Task Type  | Task Description  | Summary of Task during Phase   | Key Documents |
|- |- |- |- |
| Familiarising:   | The purpose of this task is to immerse in the type of data to be analysed to 'get a sense' of it. For example, analysts may examine example data set (from pilot studies and/or a subset of the data) with a bottom-up approach to coding; paying attention to potential themes.    | Two sets of example-data were analysed during an initial round of construction of potential analytic categories. Comparing the outcomes of these exploratory analyses we identified reoccurring analytic categories and emergent themes and compiled a draft codebook (V1). Following revisions for clarity of the analytic categories, an initial codebook was developed (V2).   | Pre-trial data set; Trial 1 data set; Experimental-SIPS data set; Codebook V2  |
| Integrating:   | The purpose of integrating is to bring together the elements that make up a research project and thinking about how they relate to one another. This process helps to identify overlaps and connections in the themes that emerged while becoming familiar with the data-sets – this might include comparing the initial coding across different data-sets by the same researcher and/or comparing the initial coding across the same data-set by different researchers.  | Analysts reviewed and refined the descriptions and examples used for each of the analytic categories (codes), updating the codebook at each iteration.     | Codebooks V3 to V5  |
| Organising:  | The task of organising aims to create structures that reflect meaningful aspects of the data in relation to project objectives. For example, initial analytic categories may be reviewed to refine the codes by comparing pre-selected categories (chosen from the relevant literature) and any emerging categories (articulated during first-pass coding); explain their similarities and differences.  | An iterative process of calibration where core analysts reviewed overlaps and connections in the themes that emerged between different analysts categorising the same data-sets  As part of this, naïve analysts were bought into to test the stability of the codebook as it emerged. Prior to analysing the main data set, training was conducted on practice datasets to improve consistency between the interpretations of the text in relation to the analytic categories as used by each analyst. Following this, discussions were cultivated to explore variations between the independent efforts of each analyst to interpret the same sets of data using the same codebook.  In addition, ICR calculations were used to monitor the intersubjective application of analytic categories. | See the calibration of the Codebooks V6-V9  |
| Exploring:   | Exploratory analysis serves to structure consideration of the patterns inherent in the data. This step may involve researchers reading the relevant segments of textual data carefully to identify units of content-meaning at the level of evocative terms (codes) set out in a Codebook (developed during the earlier stages).  If there are multiple analysts, this step also provides an opportunity to assess the stability of the analytic categories if used by multiple analysts each interpreting the same data-set (ICA). This may involve comparing the outputs of structured textual-interpretation by different analysts and, in some cases, calculating ICR for each analytic category.     | Multiple analysts used the same Codebook (10) to interpret the same set of textual data.  This round of analysis provided some initial insights about the common heuristics emerging in the reasoning provided when assessing replicability of research claims (for more detail, see the Heuristics paper [insert link]).  In addition, this round of analysis was used to provide a source of training-data for an automated process of textual-analysis to inform a quantitative research question (for details, see documentation on the reasonWAgg aggregation methods) [insert link].   To provide transparency about the reliability of this version of our analytic categories (and to help in interpreting their value identifying stable patterns in the textual-data), we calculated the ICR between our analysts for these data-sets (for more details see [here]).   | Codebook 10; Heuristics paper; reasonWAgg aggregation methods.   |
| Reflecting:   | The process of reflection includes recording analytic insights so far and reviewing the analytic processes.  | Based on the experiences during Phase 1, we learnt several lessons about both the process of dealing with large-sets of textual data and of developing a Codebook that can be used reliably by multiple analysts. In Phase 2, we will be both reviewing the CAQDAS we use and refining out Codebook prior to analysis of the data collected so far (and any future data collected).   |  See Phase 2 Documentation   |
| Interrogating:   | The process of interrogation aims at asking questions about data and the work we have done so far.  For example, this may involve identifying related units of content-meaning by investigating the patterns of co-occurrence between different coded evocative terms.   | This part of the qualitative research process will form part of Phase 2.   | See Phase 2 Documentation  |

### Interpretative categorisation of textual-platform-data by human-analysts

For each NVivo project, all responses (text-units) needed to 'coded' with all all relevant analytic categories (nodes) as detailed in the relevant codebook.

Codebook version 10 was considered a stable version of our main set of analytic categories for the purpose of Phase 1 (of these a subset use selected as 'reasonWAgg categories').

For each textual response, multiple analytic categories may be relevant Determining which combination of analytic categories apply depends on a context-based reading of each response. For example, the context for a given text-unit may include the type of question eliciting the response, and the discipline of the claim the participant is assessing.

Following training, each analyst used a copy of the base NVivo project to read an assigned set of text-units, interpete these as responses to the elicitation questions within any available context, and index each responses to the applicable nodes from the relevant codebook.

### Automating the indexing of textual-data

Once a quarter of the SCORE claims were analysed manually, the textual data from the remaining batches of claims were processed using the ‘autocode’ function in NVivo, with the manually coded data as the reference sets and a sub-set of the main codebook (the reasonWAgg Categories).

These reasonWAgg Categories included 25 nodes, as detailed [here]. Following the auto-code process, a query was run for each batch cross-referencing each the reasonWAgg categories by participant per claim. The output of this query provides an indication of how many different categories of reasoning attributed to each participants based on their responses for any given claim. These query outputs were combined and provided to the Aggregation team, see [here] for more detail.

### Calculating the inter-coder reliability (ICR) of analytic categories

Inter-coder consistency is a measure of how well one analyst’s categorisations of a given text agrees with other analysts’ categorisations of the same (or similar) texts. There are at least two overlapping approaches to measuring inter-coder consistency – inter-coder agreement (ICA) and inter-code reliability (ICR). Although accounts of ICA and ICR overlap within the literature, we understand these to be two steps in the same process. Firstly, the ICA provides a description of the extent to which different analysts tend to assign the same code to the same unit of data (Popping 2010). Secondly, the ICR “requires that two or more equally capable analysts operating in isolation from each other select the same code for the same unit of text” (Campbell et al. 2013, 297). Achieving a decent ICA is therefore a minimum requirement, however it is the degree of ICR that provides an indication that the variance in the coding by different analysts is attributable to differences among the objects coded rather than idiosyncratic coding decisions (Popping 2010). Therefore, in addition to contributing to the development of analytic categories, ICR calculations are often run on the final-round of analysis to provide an indication of how difficult it will be for independent investigators to replicate the outcomes of a given analysis following the documented methodical steps (Popping 2010). However, it is important to emphasise that, while valuable, ICR is an insufficient condition for generating robust analyses in support of generalisable research claims (Hruschka et al. 2004). Similarly, while the ability to replicate an analytic process has epistemic significance within practices where there is high degree of control on the variables (e.g., laboratory experiments) or rely on statistical inferences (such as null-hypothesis testing) it is not always helpful to extend this expectation to research practices where there is minimal control on the variables influencing the outcome of the study (Leonelli 2018; Penders, Holbrook, and de Rijcke 2019). As such, expectations of the replicability of qualitative research is often of less value than the expectation of accountability and preservation of research processes in ways that allows others to assess research claims in the context of the practices within which they emerged. As such, we were equally attentive to other stages of the research chain required to collect quality data and conduct meaningful analyses.

 In this project, ICR calculations were used in the following ways:

- calculating ICR helped identify gaps in inter-coder agreement during the calibration process where analytic categories needed to be refined (i.e., nodes being applied inconsistently due to ambiguities in the node description, background knowledge differing between analysts, etc.,)  - see codebook calibration documentation
- calculating ICR helped to determine which analytic categories to include in the reasonWAgg Aggregation - see [ICR calculation documentation].
- reporting the ICR of specific analytic categories alongside their use in project publications provided an estimate of the inter-subjective stability of the patterns these analytic categories helped to identify in the textual-data, see examples in the [Aggregation] and [Heuristics] papers.
- reporting the ICR of all categories in key codebooks as to provide context for anyone assessing our findings and any independent analysts using our analytic categories to structure a re-analysis of the same textual-data.

To facilitate the ICR calculations, data sets were structured to ensure a consistent coding scale (nominal, ordinal, interval, or ratio) such that all analysts were coding for common units of meaning. For this project, the coding scale was nominal, and the common units of meaning were consistent for each type of data-input - which, for the in-platform response, was the entire text-box (approximately one to three sentences).

There are multiple approaches to calculating an ICR score, including calculations based on mean pairwise agreement, simultaneous agreement, and majority agreement (Popping 2010, 1071). Each of these approaches have specific benefits and limitations and different approaches and accepted levels of reliability are considered appropriate in specific contexts (Hruschka et al. 2004). In each case, an adequate ICR can usually be determined at the point when multiple analysts consistently allocated the same quotations to the same general group of codes (theme), even when not agreeing on each identical code for every unit of data (e.g., MacPhail et al. 2016, 206).

In this project, ICR was computed as a Krippendorff’s alpha score for each analytic category (node). A Krippendorff’s point-estimate alpha score can be interpreted as the extent of deviation from perfect agreement by the proportion of observed disagreement to expected disagreement:

- An alpha of 1 indicates perfect agreement (there was no observed disagreement);
- An alpha of 0 indicates no agreement beyond that expected by chance alone (i.e. the amount of observed disagreement matches exactly the expected level of disagreement);
- An alpha of less than 0 indicates that the disagreement observed is greater than that expected by chance (this would indicate that there’s some systematic disagreement occurring among coders).

 To be included in the reasonWAgg aggregations, a node required either a minimum ICR of 0.66  or an ICR between two coders of at least 0.75 and a minimum overall ICR of 0.50. However, this cut-off was not required for qualitative analyses; instead the ICR will be reported (as a Krippendorff’s alpha) at a node-specific level for the relevant data-set when discussed in relation to specific research questions.

## Bibliography (tbc)

 [Dag Stenvoll, and Peter Svensson. 2011. ‘Contestable Contexts: The Transparent Anchoring of Contextualization in Text-as-Data’. Qualitative Research 11 (5): 570–86](https://doi.org/10.1177/146879411141324)

Forrester, Michael A., ed. 2010. Doing Qualitative Research in Psychology: A Practical Guide. Los Angeles; London: SAGE.

[Hsieh, Hsiu-Fang, and Sarah E. Shannon. 2005. ‘Three Approaches to Qualitative Content Analysis’. Qualitative Health Research 15 (9): 1277–88](https://doi.org/10.1177/1049732305276687)

Popping, Roel. 2000. Computer-Assisted Text Analysis. SAGE.

Saldaña, Johnny. 2016. The Coding Manual for Qualitative Researchers. 3E [Third edition]. Los Angeles ; London: SAGE.

Schwandt, Thomas A. 2001. Dictionary of Qualitative Inquiry. 2nd ed. Thousand Oaks, California: Sage Publications.
Vogt, W. Paul, and R. Burke Johnson. 2016. The Sage Dictionary of Statistics & Methodology: A Nontechnical Guide for the Social Sciences. Fifth Edition. Thousand Oaks, California: SAGE Publications, Inc.

Wästerfors, David, Marlin Åkerström, and Katarina Jacobsson. 2014. ‘Reanalysis of Qualitative Data’. In The SAGE Handbook of Qualitative Data Analysis, edited by Uwe Flick, Katie Metzler, and Wendy Scott, 467–80. London, [England]: SAGE.

Woolf, Nicholas H., and Christina Silver. 2017. Qualitative Analysis Using NVivo: The Five-Level QDA Method. Developing Qualitative Inquiry. New York: Routledge.

[1] Given these traditions, qualitative studies are often considered to rely upon a relativistic epistemology. However, qualitative methods can be, and are, used within a range of theoretical frameworks: from strong social construction to positivist style realism, as well as – more recently – various approaches that seek to side-step this dichotomy (Forrester 2010, 18–32).
