# repliCATS Phase One Qualitative Research Methods Summary

## Context for qualitative research decisions in the repliCATS project

To complement the quantitative research questions within the repliCATS project, we incorporated a qualitative approach to investigate the reasoning provided for predictions about the replicability of research claims.

In this context, our data included the textual data collected through the free-text responses that participants provided, along with numerical estimates, about the likely replicability of specific research claims (on the repliCATS platform variation on the IDEA protocol). This large data set was supplemented by interviews with a sub-set of the participants in repliCATS workshops. More detail on the data-collection process has been documented by out Elicitation and Aggregation teams. The following details are provided as an overview of our qualitative approach to analysing this textual data during Phase 1 of the project.

While qualitative and quantifiable approaches overlap up practice, this remains an influential way of differentiating between research involving non-numerical data, such as text and images (qualitative) and those (quantitative) approaches that focus on data that can be handled numerically (Schwandt 2001; Vogt and Johnson 2016). While studies are sometimes considered qualitative simply due to involving non-numerical data, there is a wide range of approaches to analysing textual data within the qualitative methods literature. For example, within the social sciences it is common to focus on the frequency of various analytic categories (including term use or instances of types-of-responses). Meanwhile, within the humanities, qualitative approaches extend beyond the frequency of terms to examine the contextualised content of textual data (including adapting hermeneutic approaches to interpreting texts or following phenomenological traditions that emphasise descriptive accounts of subjective experiences). [1]

Drawing on existing approaches, we incorporated contextualised content-analysis techniques for systematically interpreting textual data by developing rules for multiple analysts to consistently index (‘code’) content within passages of textual data in ways that supports convergent analyses of those texts. This approach identifies the appearance of content-types within text using a set of analytic categories (‘codes’), yet that goes beyond calculating the frequency of the application of such codes to interpret the underlying content of that text within the relevant context (Hsieh and Shannon 2005). In interpreting the implicit meaning of texts, we followed existing guidelines by combining provisional coding for predefined grammatical and elemental codes with an exploratory stage of indexing additional content of relevance to the research questions (Saldaña 2016). This process went beyond providing instructions for identifying the appearance of terms (e.g., plausible) in the text, to specify synonymous terms and guide interpretation of implicit references to the category of analysis (e.g., content relevant to the category of plausibility might include texts such as ‘reason to believe’, ‘seemed reasonable’, and so on). This type of structured approach aligns with a process for qualitative content-analysis that has been described as “a research technique for making replicable and valid inferences from [a set of] text to their content” (Popping 2000, 7).

This analytic process was formalised by developing a codebook specifying the inclusion and exclusion criteria for those codes most relevant to our research questions. To ensure the stability of these codes for reliably reflecting the content of the textual data collected, at an appropriate level of complexity, this codebook emerged through an iterative process where several different analysts independently analysed subsections of the data and then compared which codes each applied to the same sections of text. In addition, each of these codes was assessed for inter-coder-reliability (ICR) at various points in this process. For example, when multiple analysts used a key version of the codebook to index the same subset of the texts, the ICR of each code was calculated. The ICR results contributed to reviewing the convergence of textual interpretation between analysts. In addition, when this process included analysts naive to the codebook development, ICR calculations provided an indication of which analytic categories required further calibration and which could function as a reliable tool interpreting the text in relation to our research questions for multiple analysts.

The value of transparent process and the potential of re-analysing textual data is well-recognised within qualitative research practices even though it is not relevant for all studies (Wästerfors, Åkerström, and Jacobsson 2014; Dag Stenvoll and Peter Svensson 2011). Therefore, in addition to calibrating our codebook to support multiple analysts converging upon consistent interpretations of the text, we also documented the decisions made during the codebook development and further analysis. This includes the current documentation on how the textual data was indexed, as well as the archived versions of our codebooks, to support re-analysis by those interested in our research questions. However, given the amount and richness of the textual data collected, there would also be value in re-indexing the data with the goal of examining different research questions entirely. Either way, by documenting the indexing process, we hope that any additional analyses of this textual data will have the benefit of potential cross-references with the analysis already conducted.

## Data Collection Process

The aim of collecting qualitative data during the repliCATS project was to provide a way to analyse the reasons that participants gave about the likely replicability of published research claims in the social sciences. For each participant, we collected three types of data: a) basic demographic information (quantitative and qualitative data) b) their predictions (quantitative data) and c) information about how predictive judgements were made (qualitative data). Predictions were elicited through a [technical implementation of the IDEA Protocol](https://scholarspace.manoa.hawaii.edu/handle/10125/70666). In both local and remote IDEA groups, types b) and c) were collected using a custom-built platform for group interaction. In addition, local groups had the opportunity discussing the claims in person (in addition to entering their predictions and associated reasoning into the platform). As such, workshop facilitators took detailed notes and audio recordings of group discussions (with participant’s consent).
A subset of the participants were interviewed about their own individual reflections of experiences during the assessment of specific claims following the SIPS2019 workshop and the SIPS2020 workshop (X participants) (while, due to delays with transcription, these interviews were not analysed during Phase 1, they will remain with the data set).

## Data-processing

Following a trial and several discussions exploring options, a process was agreed upon where the Data-Management team would download data from the platform and prepare excel files that could be uploaded into our Computer-Aided Qualitative Data Analysis Software (CAQDAS), NVivo version 12. While this formatting process evolved over the course of the project, each batch included four file types:

1. J&C Files (Textual Responses by Claim): each row functions as a unit of textual data (either a justification or a comment), with a column for each of the characteristics of those responses (including a column for linking that data to the other file types) - column names/types detailed in the Aggregation Data-pipeline Overview.

2. Ratings Files (Numerical Responses by Claim): with one row for each participant per round per claim, columns were included for each question as well as for additional characteristics of those responses (including a column for linking that data to the other file types) - column names/types detailed in the Aggregation Data-pipeline Overview.

3. Claim Metadata Files: with one row per claim, columns were included for each of the characteristics of that claim (based on COS metadata for each claim: Claim id; journal; COS journal discipline; Web of science discipline) - column names/types detailed in the Aggregation Data-pipeline Overview.

4. Participant Demographic Files: with one row per participant, and columns for each of the demographic questions asked and each quiz-question answered - column names/types detailed in the Aggregation Data-pipeline Overview.

For file types 1-3, we initially chose to batch response by claim (one file per claim) for the SIPS2019 workshop, the time required to upload each file (~1min per file) prompted us to change to batching in larger sets (e.g., by workshop).The demographics file was updated progressively with each workshop, with the full set uploaded with each batch of the other files.

Depending on the data-processing pipeline, these file types required additional preparation prior to uploading into NVivo. This included, removing any #NA and ‘test’ listings, and removing duplicate listing (for example, when participants had multiple display names over the course of the project).

For the SCORE data, there were 12 batches:

| Claim Batch | Number of Claims| Number of Textual Responses |
| :----------:| :--------------:|:--------------:|:--------------:|
| 1 | 557 | 10198 |
| 2 | 219 | 3459 |
| 3 | 188 | 3489 |
| 4 | 598 | 2508 |
| 5 | 30 | 447 |
| 6 | 28 | 402 |
| 7 | 300 | 3811 |
| 8 | 143 | 2019 |
| 9 | 234 | 2947 |
| 10 | 235 | 2909 |
| 11 | 235 | 2896 |
| 12 | 234 | 3087 |
| Totals | 3001 | 38172 |

In addition to the SCORE data, we also included a batch of data collected at the SIPS2019 workshops from the groups assessing ‘known-outcome claims’ rather than SCORE claims. This was collected and processed in the same way that the SCORE claims were, with some adjustments. In terms of collection, the key difference was that each claim was assessed by multiple groups. For the file types, an additional column was added in the Metadata file for the actual results of a replication of each claim. A fifth file type was also generated for the self-reported expertise of each individual about each of the claims (one row, per participant, with columns for each claim).

Once files had been uploaded into NVivo, each file (or relevant rows within files) were indexed using the Case and Case Classification functions of this CAQDAS. These function to classify multiple units of text at a given case for further analysis (e.g., to review all responses by a group across multiple claims, or all responses for a claim across multiple groups).

Within NVivo, file types 2-4 were used to create Case Classifications. For instance, for the SCORE claims, the classifications (and the cases they were linked to) were:
- Claim_Metadata (linked to the ‘claim id’ column from the J&C file)
- Ratings_NumericalResponses_by_Claim (linked to the ‘unique id participant’ column of the J&C file).
- Participant_attributes_ ( linking the demographics file to the participant id in the ‘justifier’ column of the J&C file).

These classifications were applied manually in NVivo, and given the workflow in this project, this was a separate process to the use of 'nodes' to apply analytic categories to text units. Once both case classifications and nodes were applied to text-units, queries were run within NVivo to extract relevant subsets of data relevant to specific research questions (see Qualitative Analysis Process for more detail).

For each batch of data, an NVivo Project was created with the relevant files processed and uploaded. Following recommendations in Woolf and Silver (2017, 108), for each NVivo Project a process of distribution-merge-distribution was used. The recommended steps involved in this process can be described as follows (Woolf and Silver 2017, 110):
- A 'merge-manager' creates a base NVivo project by uploading the relevant source files, 'codebook' of nodes, and naming any cases and classifications needed for linking files.
- Each analyst is sent a copy of this file to complete a specified task (e.g., link case-classification to a file, or analyse a set of text-units using the nodes)
- Each analyst completes their task in their version of the NVivo project (the 'sub-project'), remembering to avoid adding or deleting sources or cases, or deleting or changing the names of nodes, cases, or classifications, saves their work, creates a copy of their NVivo sub-project, and sends it to the merge-manager for review.
- The merge-manager opens each submitted sub-project to confirm that each one contains all the sources in the master it was initially copied from and imports each sub-project into a master-project to be reviewed.
- An analyst meeting is held to discuss any suggested additions, modifications, or deletions to the NVivo project, as well as any sources that need to be added to the project.
- If required, the merge-manager adds any new sources to the updated master-project, makes any agreed-upon modifications, and removes any agreed-upon deletions, before copying to re-send to each analyst for the next round of analysis (and archives a copy for the project records). Once received, each analyst continues work on the newest copy of the project (archiving the old version).
- Repeat as required.

## Qualitative Analysis Process

Our principal unit of analysis was the justifications that participants provided for their prediction of the replicability of a claim. In analysing these, we explored trends in reasoning within these justifications for each claim across multiple individuals and multiple groups. In addition, we  explored trends within group-level and individual-level justifications across multiple claims.

The process of analysing this textual data involved multiple interdependent practices, however for brevity these are documented by type below.
### Developing key analytic categories

We developed a codebook that specifies the analytic categories used to interpret the textual data collected during the repliCATS project. The development of this codebook, and use of relevant analytic categories, can be summarised in terms of the qualitative analytic task types by Woolf and Silver (2017).
### Interpretative categorisation of textual-platform-data by human-analysts (using Codebook V10)
For each J&C file uploaded, each text-unit needed to be read such that all relevant analytic categories ('nodes' in NVIvo terminology) could be applied.


To do.
### Automated categorisation of textual-platform-data using 'reasonWAgg categories' based on human-analysed textual platform data

To do.

### Monitoring Inter-Coder Reliability (ICR) for Analytic Categories

 To ensure that that ICR was calculable, the data was structured to ensure a consistent coding scale (nominal, ordinal, interval, or ratio) such that all analysts were coding for common units of meaning. For this project, the coding scale was nominal, and the common units of meaning were consistent for each type of data-input - which, for the in-platform response, was the entire text-box (approximately one to three sentences).

 To do: add details of ICR process.

### Computer-aided Qualitative Analysis Software (CAQDAS) troubleshooting

To do.

### Integration of qualitative analysis data into repliCATS aggregation methods

To do: copy figure from Aggregation Documentation

## Bibliography

 [Dag Stenvoll, and Peter Svensson. 2011. ‘Contestable Contexts: The Transparent Anchoring of Contextualization in Text-as-Data’. Qualitative Research 11 (5): 570–86](https://doi.org/10.1177/146879411141324)

Forrester, Michael A., ed. 2010. Doing Qualitative Research in Psychology: A Practical Guide. Los Angeles; London: SAGE.

[Hsieh, Hsiu-Fang, and Sarah E. Shannon. 2005. ‘Three Approaches to Qualitative Content Analysis’. Qualitative Health Research 15 (9): 1277–88](https://doi.org/10.1177/1049732305276687)

Popping, Roel. 2000. Computer-Assisted Text Analysis. SAGE.

Saldaña, Johnny. 2016. The Coding Manual for Qualitative Researchers. 3E [Third edition]. Los Angeles ; London: SAGE.

Schwandt, Thomas A. 2001. Dictionary of Qualitative Inquiry. 2nd ed. Thousand Oaks, California: Sage Publications.
Vogt, W. Paul, and R. Burke Johnson. 2016. The Sage Dictionary of Statistics & Methodology: A Nontechnical Guide for the Social Sciences. Fifth Edition. Thousand Oaks, California: SAGE Publications, Inc.

Wästerfors, David, Marlin Åkerström, and Katarina Jacobsson. 2014. ‘Reanalysis of Qualitative Data’. In The SAGE Handbook of Qualitative Data Analysis, edited by Uwe Flick, Katie Metzler, and Wendy Scott, 467–80. London, [England]: SAGE.

Woolf, Nicholas H., and Christina Silver. 2017. Qualitative Analysis Using NVivo: The Five-Level QDA Method. Developing Qualitative Inquiry. New York: Routledge.

[1] Given these traditions, qualitative studies are often considered to rely upon a relativistic epistemology. However, qualitative methods can be, and are, used within a range of theoretical frameworks: from strong social construction to positivist style realism, as well as – more recently – various approaches that seek to side-step this dichotomy (Forrester 2010, 18–32).
